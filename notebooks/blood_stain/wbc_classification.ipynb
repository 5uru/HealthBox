{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "import timm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, average_precision_score, precision_recall_fscore_support,auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download dataset\n",
    "! curl -L -o ~/peripheral-blood-cell.zip\\\n",
    "    https://www.kaggle.com/api/v1/datasets/download/bzhbzh35/peripheral-blood-cell"
   ],
   "id": "f45332d6bc805496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! unzip peripheral-blood-cell.zip",
   "id": "24d895c47273bbd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Dataset path\n",
    "dataset_dir = '/root/PBC_dataset_normal_DIB_224/PBC_dataset_normal_DIB_224'\n",
    "\n",
    "# Class names\n",
    "class_names = ['basophil', 'eosinophil', 'erythroblast', 'ig', 'lymphocyte', 'monocyte', 'neutrophil', 'platelet']\n",
    "\n",
    "# Data augmentation and normalization\n",
    "transform = {\n",
    "        'train': transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "}\n",
    "\n",
    "# Load the full dataset without split\n",
    "full_dataset = datasets.ImageFolder(dataset_dir, transform=transform['train'])\n",
    "\n",
    "# Split indices for train, validation, and test (80% train, 10% validation, 10% test)\n",
    "train_idx, test_idx = train_test_split(list(range(len(full_dataset))), test_size=0.2, stratify=full_dataset.targets)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.125, stratify=[full_dataset.targets[i] for i in train_idx])\n",
    "\n",
    "# Create subsets\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset = Subset(full_dataset, val_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')"
   ],
   "id": "85345c6408a17b60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Update transforms for validation and test datasets (since they share the same normalization)\n",
    "val_dataset.dataset.transform = transform['test']\n",
    "test_dataset.dataset.transform = transform['test']\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate class weights for balanced training\n",
    "def calculate_class_weights(dataset):\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        targets = np.array([dataset.dataset.targets[i] for i in dataset.indices])\n",
    "    else:\n",
    "        targets = np.array(dataset.targets)\n",
    "\n",
    "    class_sample_count = np.bincount(targets)\n",
    "    class_weights = 1. / class_sample_count\n",
    "\n",
    "    return torch.from_numpy(class_weights).float()\n",
    "\n",
    "# Create a weighted sampler for oversampling minority classes\n",
    "def create_weighted_sampler(dataset):\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        targets = np.array([dataset.dataset.targets[i] for i in dataset.indices])\n",
    "    else:\n",
    "        targets = np.array(dataset.targets)\n",
    "\n",
    "    class_weights = calculate_class_weights(dataset)\n",
    "    samples_weights = np.array([class_weights[t] for t in targets])\n",
    "    sampler = WeightedRandomSampler(samples_weights, len(samples_weights))\n",
    "\n",
    "    return sampler\n",
    "\n",
    "# Use WeightedRandomSampler in the DataLoader for balanced sampling\n",
    "train_sampler = create_weighted_sampler(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'Train DataLoader size: {len(train_loader.dataset)}')\n",
    "print(f'Validation DataLoader size: {len(val_loader.dataset)}')\n",
    "print(f'Test DataLoader size: {len(test_loader.dataset)}')\n",
    "\n",
    "# Calculate class weights for CrossEntropyLoss\n",
    "class_weights = calculate_class_weights(full_dataset)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# Print class weights\n",
    "print(f'Class Weights: {class_weights}')"
   ],
   "id": "e3558565803e5d82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to visualize class distribution and total sample count from batches\n",
    "def visualize_batch_class_distribution(data_loader, num_batches=10):\n",
    "    class_count = np.zeros(len(class_names))\n",
    "    total_samples = 0\n",
    "\n",
    "    # Loop through the specified number of batches\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        total_samples += labels.size(0)  # Add the number of samples in the current batch\n",
    "        class_count += np.bincount(labels.numpy(), minlength=len(class_names))\n",
    "\n",
    "    # Print total number of samples processed\n",
    "    print(f\"Total number of samples processed in {num_batches} batches: {total_samples}\")\n",
    "\n",
    "    # Plot the batch distribution\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=class_names, y=class_count)\n",
    "    plt.title(f\"Class Distribution in {num_batches} Sampled Batches\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the class distribution over multiple batches and show the total number of samples\n",
    "visualize_batch_class_distribution(train_loader, num_batches=50)\n"
   ],
   "id": "144ab1b1cccc70fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to display an image tensor\n",
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "# Sample 'num_samples' images per class\n",
    "def plot_class_samples(dataset, class_names, num_samples=5):\n",
    "    num_classes = len(class_names)\n",
    "    samples_per_class = {class_name: [] for class_name in class_names}\n",
    "\n",
    "    # Go through dataset and store 'num_samples' samples per class\n",
    "    for img, label in dataset:\n",
    "        class_name = class_names[label]\n",
    "        if len(samples_per_class[class_name]) < num_samples:\n",
    "            samples_per_class[class_name].append((img, class_name))\n",
    "        if all(len(v) == num_samples for v in samples_per_class.values()):\n",
    "            break  # Stop once we have 'num_samples' samples per class\n",
    "\n",
    "    # Plot the images in rows (one row per class)\n",
    "    fig, axes = plt.subplots(nrows=num_classes, ncols=num_samples, figsize=(15, 3*num_classes))\n",
    "\n",
    "    for row, class_name in enumerate(class_names):\n",
    "        for col, (img, _) in enumerate(samples_per_class[class_name]):\n",
    "            ax = axes[row, col]\n",
    "            ax.imshow(imshow(img))\n",
    "            ax.axis('off')\n",
    "\n",
    "        # Set the title for the first image of each row\n",
    "        axes[row, 0].set_title(class_name, fontsize=14, pad=20, loc='left')\n",
    "\n",
    "    # Adjust layout to give more space for titles\n",
    "    plt.subplots_adjust(left=0.15, hspace=0.5, wspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Sample and plot 5 images for each class\n",
    "plot_class_samples(full_dataset, class_names, num_samples=5)"
   ],
   "id": "98e354beb845d31b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL_NAME = 'tf_efficientnet_b0'\n",
    "NUM_CLASSES =len(class_names)\n",
    "# Model Creation\n",
    "# Load the pre-trained model from timm\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=0) # num_classes=0 removes the classifier\n",
    "\n",
    "# Get the number of input features for the final layer\n",
    "num_in_features = model.num_features\n",
    "\n",
    "# Add our custom classification head\n",
    "model.classifier = nn.Sequential(\n",
    "        nn.BatchNorm1d(num_in_features),\n",
    "        nn.Linear(num_in_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ],
   "id": "afe5d80bf489ff30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training Phase 1: Train only the classifier head\n",
    "# Freeze all layers in the base model\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS_INITIAL = 10\n",
    "NUM_EPOCHS_FINE_TUNE = 10\n",
    "LEARNING_RATE_INITIAL = 1e-3\n",
    "LEARNING_RATE_FINE_TUNE = 1e-5\n",
    "# --- 4. Validation Function ---\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # No need to track gradients for validation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ],
   "id": "15ff80dee2fc8545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training Phase 1: Train only the classifier head\n",
    "print(\"\\n--- Phase 1: Training the classifier head ---\")\n",
    "# Freeze base model layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze classifier head\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE_INITIAL)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_INITIAL):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_INITIAL} -> \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(\"  -> New best model saved!\")\n",
    "\n",
    "# Load best model weights from Phase 1 before fine-tuning\n",
    "model.load_state_dict(best_model_wts)"
   ],
   "id": "2262bf4dd0e1c230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training Phase 2: Fine-tune the whole model\n",
    "import time\n",
    "print(\"\\n--- Phase 2: Fine-tuning the entire model ---\")\n",
    "# Unfreeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Re-create optimizer with a lower learning rate\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE_FINE_TUNE)\n",
    "\n",
    "best_val_loss = float('inf') # Reset best loss for fine-tuning phase\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINE_TUNE):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_FINE_TUNE} -> \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(\"  -> New best model saved!\")"
   ],
   "id": "6bcaae71becf397a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#  Run Inference on Test Set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"\\nRunning inference on the test set...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Inference complete.\")\n",
    "\n",
    "# --- 5. Generate and Print Classification Report ---\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "report = classification_report(all_labels, all_preds, target_names=class_names)\n",
    "print(report)\n",
    "\n",
    "# --- 6. Generate and Plot Confusion Matrix ---\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for WBC Classification')\n",
    "plt.show()"
   ],
   "id": "bc9cba93b2b7be61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(\"\\nFinished Training. Loading best model weights.\")\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "print(\"Attempting to save the model now...\")\n",
    "\n",
    "torch.save(model.state_dict(), 'wbc_classifier_efficientnet_b0_best.pth')\n",
    "print(\"Best model saved to wbc_classifier_efficientnet_b0_best.pth\")"
   ],
   "id": "5fa217097a354a85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
